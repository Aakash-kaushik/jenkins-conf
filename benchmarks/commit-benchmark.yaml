# Block for general settings.
library: general
settings:
  # A short timeout, since we want to run this nightly.
  timeout: 1000
  database: 'reports/commit.db'
  # These shouldn't matter with the newer js interface.
  keepReports: 20
  bootstrap: 10
  libraries: ['mlpack'] # Only mlpack in this configuration.
  version: ['trunk']
  irc: ['#mlpack', 'benchmark', 'irc.freenode.net'] # channel, nickname, server

---
library: mlpack
methods:
  ALLKNN:
    run: ['timing', 'watch']
    watch: ['neighbor_search', 'neighbor_search_impl', 'neighbor_search_rules',
            'nearest_neighbor_rules_impl', 'ns_traversal_info', 'unmap',
            'furthest_neighbor_sort', 'nearest_neighbor_sort']
    script: methods/mlpack/allknn.py
    format: [csv, txt]
    iteration: 1
    datasets:
      - files: ['datasets/covtype.csv', 'datasets/corel-histogram.csv']
        options: '-k 3'

      - files: ['datasets/covtype.csv', 'datasets/corel-histogram.csv']
        options: '-k 3 --single --cover_tree'

      - files: ['datasets/covtype.csv', 'datasets/corel-histogram.csv']
        options: '-k 3 --cover_tree'

  ALLKFN:
    run: ['timing', 'watch']
    watch: ['neighbor_search', 'neighbor_search_impl', 'neighbor_search_rules',
            'nearest_neighbor_rules_impl', 'ns_traversal_info', 'unmap',
            'furthest_neighbor_sort', 'nearest_neighbor_sort']
    script: methods/mlpack/allkfn.py
    format: [csv, txt]
    iteration: 1
    datasets:
      - files: ['datasets/covtype.csv', 'datasets/corel-histogram.csv']
        options: '-k 3'

      - files: ['datasets/covtype.csv', 'datasets/corel-histogram.csv']
        options: '-k 3 --single --cover_tree'

      - files: ['datasets/covtype.csv', 'datasets/corel-histogram.csv']
        options: '-k 3 --cover_tree'

      - files: ['datasets/covtype.csv', 'datasets/corel-histogram.csv']
        options: '-k 3 --cover_tree'

      - files: ['datasets/covtype.csv', 'datasets/corel-histogram.csv']
        options: '-k 3 --single --r_tree'

      - files: ['datasets/covtype.csv', 'datasets/corel-histogram.csv']
        options: '-k 3 --r_tree'

  ALLKRANN:
    run: ['timing', 'watch']
    watch: ['neighbor_search', 'neighbor_search_impl', 'neighbor_search_rules',
            'nearest_neighbor_rules_impl', 'ns_traversal_info', 'unmap',
            'furthest_neighbor_sort', 'nearest_neighbor_sort']
    script: methods/mlpack/allkrann.py
    format: [csv, txt]
    iteration: 1
    datasets:
      - files: ['datasets/covtype.csv', 'datasets/corel-histogram.csv']
        options: '-k 3'

      - files: ['datasets/covtype.csv', 'datasets/corel-histogram.csv']
        options: '-k 3 --alpha 0.9'

      - files: ['datasets/covtype.csv', 'datasets/corel-histogram.csv']
        options: '-k 3 --alpha 0.9 --tau 10'

      - files: ['datasets/covtype.csv', 'datasets/corel-histogram.csv']
        options: '-k 3 --sample_at_leaves'

      - files: ['datasets/covtype.csv', 'datasets/corel-histogram.csv']
        options: '-k 3 --sample_at_leaves --single'

      - files: ['datasets/covtype.csv', 'datasets/corel-histogram.csv']
        options: '-k 3 --first_leaf_exact'

      - files: ['datasets/covtype.csv', 'datasets/corel-histogram.csv']
        options: '-k 3 --first_leaf_exact --single'

      - files: ['datasets/covtype.csv', 'datasets/corel-histogram.csv']
        options: '-k 3 --cover_tree'

      - files: ['datasets/covtype.csv', 'datasets/corel-histogram.csv']
        options: '-k 3 --cover_tree --single'

  DecisionStump:
    run: ['timing', 'watch']
    script: methods/mlpack/decision_stump.py
    format: [csv, txt]
    iteration: 1
    datasets:
      - files: [ ['datasets/iris_train.csv', 'datasets/iris_test.csv'],
                 ['datasets/optdigits_train.csv', 'datasets/optdigits_test.csv'] ]

      - files: [ ['datasets/iris_train.csv', 'datasets/iris_test.csv'],
                 ['datasets/optdigits_train.csv', 'datasets/optdigits_test.csv'] ]
        options: '-b 20'

  DET:
    run: ['timing', 'watch']
    watch: ['dtree']
    script: methods/mlpack/det.py
    format: [csv, txt]
    iteration: 1
    datasets:
      - files: ['datasets/iris.csv', 'datasets/cloud.csv',
                ['datasets/diabetes_X.csv', 'datasets/diabetes_y.csv'] ]

      - files: ['datasets/iris.csv', 'datasets/cloud.csv',
                ['datasets/diabetes_X.csv', 'datasets/diabetes_y.csv'] ]
        options: '-f 0'

  EMST:
    run: ['timing', 'watch']
    watch: ['dtb', 'edge_pair', 'union_find']
    script: methods/mlpack/emst.py
    format: [csv, txt]
    iteration: 1
    datasets:
      - files: ['datasets/covtype.csv', 'datasets/corel-histogram.csv']

      - files: ['datasets/covtype.csv', 'datasets/corel-histogram.csv']
        options: '--naive'

      - files: ['datasets/covtype.csv', 'datasets/corel-histogram.csv']
        options: '--leaf_size 20'

  FastMKS:
    run: ['timing', 'watch']
    script: methods/mlpack/fastmks.py
    format: [csv, txt]
    iteration: 1
    datasets:
      - files: ['datasets/optdigits.csv', 'datasets/cloud.csv']
        options: '-k 3 --kernel linear'

      - files: ['datasets/optdigits.csv', 'datasets/cloud.csv']
        options: '-k 3 --kernel linear --single'

      - files: ['datasets/optdigits.csv', 'datasets/cloud.csv']
        options: '-k 3 --kernel gaussian'

      - files: ['datasets/optdigits.csv', 'datasets/cloud.csv']
        options: '-k 3 --kernel gaussian --single'

      - files: ['datasets/optdigits.csv', 'datasets/cloud.csv']
        options: '-k 3 --kernel polynomial --degree 2'

      - files: ['datasets/optdigits.csv', 'datasets/cloud.csv']
        options: '-k 3 --kernel polynomial --degree 2 --single'

  GMM:
    run: ['timing', 'watch']
    watch: ['diagonal_constraint', 'eigenvalue_ratio_constraint',
            'em_fit', 'no_constraint', 'positive_definite_constraint']
    script: methods/mlpack/gmm.py
    format: [csv, txt]
    iteration: 1
    datasets:
      - files: ['datasets/iris.csv', 'datasets/wine.csv']
        options: '--gaussians 3 --seed 42'

      - files: ['datasets/iris.csv', 'datasets/wine.csv']
        options: '--gaussians 3 --seed 42 --no_force_positive'

      - files: ['datasets/iris.csv', 'datasets/wine.csv']
        options: '--gaussians 3 --seed 42 --noise 0.1'

      - files: ['datasets/iris.csv', 'datasets/wine.csv']
        options: '--gaussians 3 --seed 42 --trials 20'

      - files: ['datasets/iris.csv', 'datasets/wine.csv']
        options: '--gaussians 3 --seed 42 --trials 20 --refined_start'

      - files: ['datasets/optdigits.csv']
        options: '--gaussians 10 --seed 42'

      - files: ['datasets/optdigits.csv']
        options: '--gaussians 10 --seed 42 --no_force_positive'

      - files: ['datasets/optdigits.csv']
        options: '--gaussians 10 --seed 42 --noise 0.1'

      - files: ['datasets/optdigits.csv']
        options: '--gaussians 10 --seed 42 --trials 20'

      - files: ['datasets/optdigits.csv']
        options: '--gaussians 10 --seed 42 --trials 20 --refined_start'

  HMMTRAIN:
    run: ['timing', 'watch']
    script: methods/mlpack/hmm_train.py
    format: [csv, txt]
    iteration: 1
    datasets:
      - files: ['datasets/artificial_2DSignal.csv']
        options: '--type gaussian --states 20 --seed 42'

      - files: ['datasets/artificial_1DSignal.csv']
        options: '--type discrete --states 20 --seed 42'

  HMMGENERATE:
    run: ['timing', 'watch']
    script: methods/mlpack/hmm_generate.py
    format: [csv, txt, xml]
    iteration: 1
    datasets:
      - files: ['datasets/artificial_2DSignal_hmm.xml', 'datasets/artificial_1DSignal_hmm.xml']
        options: '--length 10000'

  HMMLOGLIK:
    run: ['timing', 'watch']
    script: methods/mlpack/hmm_loglik.py
    format: [csv, txt, xml]
    iteration: 1
    datasets:
      - files: [ ['datasets/artificial_2DSignal.csv', 'datasets/artificial_2DSignal_hmm.xml'],
                 ['datasets/artificial_1DSignal.csv', 'datasets/artificial_1DSignal_hmm.xml'] ]

  HMMVITERBI:
    run: ['timing', 'watch']
    script: methods/mlpack/hmm_viterbi.py
    format: [csv, txt, xml]
    iteration: 1
    datasets:
      - files: [ ['datasets/artificial_2DSignal.csv', 'datasets/artificial_2DSignal_hmm.xml'],
                 ['datasets/artificial_1DSignal.csv', 'datasets/artificial_1DSignal_hmm.xml'] ]

  KPCA:
    run: ['timing', 'watch']
    watch: ['naive_method', 'nystroem_method']
    script: methods/mlpack/kernel_pca.py
    format: [csv, txt]
    iteration: 1
    datasets:
      - files: ['datasets/circle_data.csv', 'datasets/abalone.csv']
        options: '--kernel linear --new_dimensionality 2'

      - files: ['datasets/circle_data.csv', 'datasets/abalone.csv']
        options: '--kernel gaussian --new_dimensionality 2'

      - files: ['datasets/circle_data.csv', 'datasets/abalone.csv']
        options: '--kernel polynomial --degree 2 --new_dimensionality 2'

      - files: ['datasets/circle_data.csv', 'datasets/abalone.csv']
        options: '--kernel hyptan --new_dimensionality 2'

      - files: ['datasets/circle_data.csv', 'datasets/abalone.csv']
        options: '--kernel linear --new_dimensionality 2 --nystroem_method --sampling random'

      - files: ['datasets/circle_data.csv', 'datasets/abalone.csv']
        options: '--kernel gaussian --new_dimensionality 2 --nystroem_method --sampling random'

      - files: ['datasets/circle_data.csv', 'datasets/abalone.csv']
        options: '--kernel polynomial --degree 2 --new_dimensionality 2 --nystroem_method --sampling random'

      - files: ['datasets/circle_data.csv', 'datasets/abalone.csv']
        options: '--kernel hyptan --degree 2 --new_dimensionality 2 --nystroem_method --sampling random'

      - files: ['datasets/circle_data.csv', 'datasets/abalone.csv']
        options: '--kernel linear --new_dimensionality 2 --nystroem_method --sampling kmeans'

      - files: ['datasets/circle_data.csv', 'datasets/abalone.csv']
        options: '--kernel gaussian --new_dimensionality 2 --nystroem_method --sampling kmeans'

      - files: ['datasets/circle_data.csv', 'datasets/abalone.csv']
        options: '--kernel polynomial --degree 2 --new_dimensionality 2 --nystroem_method --sampling kmeans'

      - files: ['datasets/circle_data.csv', 'datasets/abalone.csv']
        options: '--kernel hyptan --degree 2 --new_dimensionality 2 --nystroem_method --sampling kmeans'

  KMEANS:
    run: ['timing', 'watch']
    watch: ['max_variance_new_cluster', 'random_partition', 'refined_start', 'allow_empty_clusters']
    script: methods/mlpack/kmeans.py
    format: [csv, txt, arff]
    iteration: 1
    datasets:
      - files: [ ['datasets/cloud.csv', 'datasets/cloud_centroids.csv'] ]
        options: '--clusters 5'

      - files: [ ['datasets/cloud.csv', 'datasets/cloud_centroids.csv'] ]
        options: '--clusters 5 --allow_empty_clusters'

      - files: [ ['datasets/USCensus1990.csv', 'datasets/USCensus1990.csv'] ]
        options: '--clusters 6'

      - files: [ ['datasets/USCensus1990.csv', 'datasets/USCensus1990.csv'] ]
        options: '--clusters 6 --allow_empty_clusters'

  LARS:
    run: ['timing', 'watch']
    script: methods/mlpack/lars.py
    format: [csv, txt]
    iteration: 1
    datasets:
      - files: [ ['datasets/diabetes_X.csv', 'datasets/diabetes_y.csv'],
                 ['datasets/madelon_X.csv', 'datasets/madelon_y.csv'] ]
        options: '--lambda1 0.01'

      - files: [ ['datasets/diabetes_X.csv', 'datasets/diabetes_y.csv'],
                 ['datasets/madelon_X.csv', 'datasets/madelon_y.csv'] ]
        options: '--lambda1 0.01 --lambda2 0.005'

      - files: [ ['datasets/diabetes_X.csv', 'datasets/diabetes_y.csv'],
                 ['datasets/madelon_X.csv', 'datasets/madelon_y.csv'] ]
        options: '--lambda1 0.01 --use_cholesky'

      - files: [ ['datasets/diabetes_X.csv', 'datasets/diabetes_y.csv'],
                 ['datasets/madelon_X.csv', 'datasets/madelon_y.csv'] ]
        options: '--lambda1 0.01 --lambda2 0.005 --use_cholesky'

  LinearRegression:
    run: ['timing', 'watch']
    script: methods/mlpack/linear_regression.py
    format: [csv, txt]
    iteration: 1
    datasets:
      - files: [ ['datasets/diabetes_X.csv'], ['datasets/cosExp_X.csv'],
                 ['datasets/madelon_train.csv', 'datasets/madelon_test.csv'] ]

  LocalCoordinateCoding:
    run: ['timing', 'watch']
    watch: ['lcc']
    script: methods/mlpack/local_coordinate_coding.py
    format: [csv, txt]
    iteration: 1
    datasets:
      - files: ['datasets/pendigits.csv']
        options: '--atoms 12 --seed 42'

      - files: ['datasets/pendigits.csv']
        options: '--atoms 12 --seed 42 --normalize'

      - files: ['datasets/pendigits.csv']
        options: '--atoms 12 --seed 42 --normalize --lambda 0.1'

  LSH:
    run: ['timing', 'watch']
    script: methods/mlpack/lsh.py
    format: [csv, txt]
    iteration: 1
    datasets:
      - files: ['datasets/cloud.csv', 'datasets/corel-histogram.csv']
        options: '-k 3 --seed 42'

      - files: ['datasets/cloud.csv', 'datasets/corel-histogram.csv']
        options: '-k 3 --seed 42 --bucket_size 50'

      - files: ['datasets/cloud.csv', 'datasets/corel-histogram.csv']
        options: '-k 3 --seed 42 --tables 40'

  NMF:
    run: ['timing', 'watch']
    watch: ['amf', 'simple_residue_termination', 'simple_tolerance_termination',
            'validation_rmse_termination', 'incomplete_incremental_termination',
            'complete_incremental_termination', 'averge_init', 'random_acol_init',
            'random_init']
    script: methods/mlpack/nmf.py
    format: [csv, txt]
    iteration: 1
    datasets:
      - files: ['datasets/isolet.csv']
        options: '--rank 6 --seed 42 --update_rules multdist'

      - files: ['datasets/isolet.csv']
        options: '--rank 6 --seed 42 --update_rules multdiv'

      - files: ['datasets/isolet.csv']
        options: '--rank 6 --seed 42 --update_rules als'

  PCA:
    run: ['timing', 'watch']
    script: methods/mlpack/pca.py
    format: [csv, txt]
    iteration: 1
    datasets:
      - files: ['datasets/iris.csv', 'datasets/wine.csv',
                'datasets/cities.csv', 'datasets/diabetes_X.csv']

      - files: ['datasets/iris.csv', 'datasets/wine.csv',
                'datasets/cities.csv', 'datasets/diabetes_X.csv']
        options: '--new_dimensionality 2'

      - files: ['datasets/iris.csv', 'datasets/wine.csv',
                'datasets/cities.csv', 'datasets/diabetes_X.csv']
        options: '--var_to_retain 0.7 --scale'

  PERCEPTRON:
    run: ['timing', 'watch']
    watch: ['simple_weight_update', 'random_init', 'zero_init']
    script: methods/mlpack/perceptron.py
    format: [csv, txt, arff]
    iteration: 1
    datasets:
      - files: [ ['datasets/iris_train.csv', 'datasets/iris_test.csv', 'datasets/iris_labels.csv'],
                 ['datasets/oilspill_train.csv', 'datasets/oilspill_test.csv', 'datasets/oilspill_labels.csv'],
                 ['datasets/ecoli_train.csv', 'datasets/ecoli_test.csv', 'datasets/ecoli_labels.csv'] ]
        options: '--iteration 1000'

  RANGESEARCH:
    run: ['timing', 'watch']
    watch: ['range_search']
    script: methods/mlpack/range_search.py
    format: [csv, txt]
    iteration: 1
    datasets:
      - files: ['datasets/wine.csv', 'datasets/cloud.csv',
                'datasets/corel-histogram.csv']
        options: '--max 0.02'

      - files: ['datasets/wine.csv', 'datasets/cloud.csv',
                'datasets/corel-histogram.csv']
        options: '--max 0.02 --single'

      - files: ['datasets/wine.csv', 'datasets/cloud.csv',
                'datasets/corel-histogram.csv']
        options: '--max 0.02 --single --cover_tree'

      - files: ['datasets/wine.csv', 'datasets/cloud.csv',
                'datasets/corel-histogram.csv']
        options: '--max 0.02 --cover_tree'

      - files: ['datasets/wine.csv', 'datasets/cloud.csv',
                'datasets/corel-histogram.csv']
        options: '--max 0.02 --naive'

      - files: ['datasets/wine.csv', 'datasets/cloud.csv',
                'datasets/corel-histogram.csv']
        options: '--max 0.02 --min 0.01'

      - files: ['datasets/wine.csv', 'datasets/cloud.csv',
                'datasets/corel-histogram.csv']
        options: '--max 0.02 --min 0.01 --single'

      - files: ['datasets/wine.csv', 'datasets/cloud.csv',
                'datasets/corel-histogram.csv']
        options: '--max 0.02 --min 0.01 --cover_tree'

      - files: ['datasets/wine.csv', 'datasets/cloud.csv',
                'datasets/corel-histogram.csv']
        options: '--max 0.02 --min 0.01 --single --cover_tree'

      - files: ['datasets/wine.csv', 'datasets/cloud.csv',
                'datasets/corel-histogram.csv']
        options: '--max 0.02 --min 0.01 --naive'

  SparseCoding:
    run: ['timing', 'watch']
    watch: ['data_dependent_random_initializer', 'nothing_initializer',
            'random_initializer', 'sparse_coding']
    script: methods/mlpack/sparse_coding.py
    format: [csv, txt]
    iteration: 1
    datasets:
      - files: ['datasets/pendigits.csv']
        options: '--atoms 12 --seed 42 --max_iteration 100'

      - files: ['datasets/pendigits.csv']
        options: '--atoms 12 --seed 42'

      - files: ['datasets/pendigits.csv']
        options: '--atoms 12 --seed 42 --normalize'

      - files: ['datasets/pendigits.csv']
        options: '--atoms 12 --seed 42 --lambda1 0.01 --lambda2 0.005'
